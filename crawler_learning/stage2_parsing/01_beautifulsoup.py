"""
ç¬¬äºŒé˜¶æ®µ - BeautifulSoupç½‘é¡µè§£æ

BeautifulSoupæ˜¯Pythonæœ€æµè¡Œçš„HTMLè§£æåº“ï¼Œç®€å•æ˜“ç”¨
"""

from bs4 import BeautifulSoup
import requests

# ==================== 1. HTMLåŸºç¡€çŸ¥è¯† ====================

def html_basics():
    """
    HTMLåŸºç¡€çŸ¥è¯†é€Ÿæˆ
    """
    print("=" * 60)
    print("1. HTMLåŸºç¡€çŸ¥è¯†")
    print("=" * 60)
    
    print("""
HTMLï¼ˆè¶…æ–‡æœ¬æ ‡è®°è¯­è¨€ï¼‰æ˜¯ç½‘é¡µçš„ç»“æ„

ã€åŸºæœ¬ç»“æ„ã€‘
<html>
  <head>
    <title>ç½‘é¡µæ ‡é¢˜</title>
  </head>
  <body>
    <div class="container">
      <h1 id="title">è¿™æ˜¯æ ‡é¢˜</h1>
      <p class="content">è¿™æ˜¯æ®µè½</p>
      <a href="https://example.com">é“¾æ¥</a>
    </div>
  </body>
</html>

ã€å¸¸ç”¨æ ‡ç­¾ã€‘
- <div>: å—çº§å®¹å™¨
- <span>: è¡Œå†…å®¹å™¨
- <a>: é“¾æ¥ (å±æ€§: href)
- <img>: å›¾ç‰‡ (å±æ€§: src, alt)
- <p>: æ®µè½
- <h1>-<h6>: æ ‡é¢˜
- <ul>, <li>: åˆ—è¡¨
- <table>, <tr>, <td>: è¡¨æ ¼

ã€é‡è¦å±æ€§ã€‘
- class: CSSç±»åï¼ˆä¸€ä¸ªå…ƒç´ å¯ä»¥æœ‰å¤šä¸ªclassï¼‰
- id: å”¯ä¸€æ ‡è¯†ç¬¦
- href: é“¾æ¥åœ°å€
- src: èµ„æºåœ°å€ï¼ˆå›¾ç‰‡ã€è„šæœ¬ï¼‰

ã€çˆ¬è™«å…³æ³¨ç‚¹ã€‘
1. æ‰¾åˆ°æ•°æ®åœ¨å“ªä¸ªæ ‡ç­¾é‡Œ
2. é€šè¿‡classæˆ–idå®šä½å…ƒç´ 
3. æå–æ ‡ç­¾å†…çš„æ–‡æœ¬æˆ–å±æ€§
    """)


# ==================== 2. BeautifulSoupåŸºç¡€ ====================

def bs4_basics():
    """
    BeautifulSoupåŸºç¡€ç”¨æ³•
    """
    print("=" * 60)
    print("2. BeautifulSoupåŸºç¡€")
    print("=" * 60)
    
    # ç¤ºä¾‹HTML
    html = """
    <html>
        <head>
            <title>çˆ¬è™«å­¦ä¹ ç½‘ç«™</title>
        </head>
        <body>
            <div class="header">
                <h1 id="main-title">æ¬¢è¿å­¦ä¹ çˆ¬è™«</h1>
            </div>
            <div class="content">
                <p class="intro">Pythonæ˜¯æœ€å¥½çš„çˆ¬è™«è¯­è¨€</p>
                <p class="intro">BeautifulSoupéå¸¸å¥½ç”¨</p>
                <a href="https://www.python.org">Pythonå®˜ç½‘</a>
                <a href="https://www.github.com">GitHub</a>
            </div>
            <div class="footer">
                <span>è”ç³»æˆ‘ä»¬: admin@example.com</span>
            </div>
        </body>
    </html>
    """
    
    # åˆ›å»ºBeautifulSoupå¯¹è±¡
    # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šHTMLå­—ç¬¦ä¸²
    # ç¬¬äºŒä¸ªå‚æ•°ï¼šè§£æå™¨ï¼ˆhtml.parseræ˜¯Pythonå†…ç½®çš„ï¼‰
    soup = BeautifulSoup(html, 'html.parser')
    
    # 1. è·å–æ ‡é¢˜
    title = soup.title
    print(f"âœ… æ ‡é¢˜æ ‡ç­¾: {title}")
    print(f"âœ… æ ‡é¢˜æ–‡æœ¬: {title.string}")
    
    # 2. æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ ‡ç­¾
    h1 = soup.find('h1')
    print(f"âœ… ç¬¬ä¸€ä¸ªh1: {h1.string}")
    
    # 3. é€šè¿‡idæŸ¥æ‰¾
    main_title = soup.find(id='main-title')
    print(f"âœ… IDæŸ¥æ‰¾: {main_title.string}")
    
    # 4. é€šè¿‡classæŸ¥æ‰¾ï¼ˆæ³¨æ„ï¼šclassæ˜¯Pythonå…³é”®å­—ï¼Œéœ€è¦ç”¨class_ï¼‰
    intro = soup.find(class_='intro')
    print(f"âœ… ClassæŸ¥æ‰¾: {intro.string}")
    
    # 5. æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ ‡ç­¾
    all_p = soup.find_all('p')
    print(f"âœ… æ‰€æœ‰pæ ‡ç­¾æ•°é‡: {len(all_p)}")
    for i, p in enumerate(all_p, 1):
        print(f"   {i}. {p.string}")
    
    # 6. æå–é“¾æ¥
    all_links = soup.find_all('a')
    print(f"âœ… æ‰€æœ‰é“¾æ¥:")
    for link in all_links:
        text = link.string
        url = link.get('href')  # æˆ– link['href']
        print(f"   - {text}: {url}")
    
    print()


# ==================== 3. æŸ¥æ‰¾æ–¹æ³•è¯¦è§£ ====================

def find_methods():
    """
    find() å’Œ find_all() è¯¦è§£
    """
    print("=" * 60)
    print("3. æŸ¥æ‰¾æ–¹æ³•è¯¦è§£")
    print("=" * 60)
    
    html = """
    <div class="container">
        <div class="article" id="article-1">
            <h2>æ–‡ç« æ ‡é¢˜1</h2>
            <p class="author">ä½œè€…: å¼ ä¸‰</p>
            <p class="content">è¿™æ˜¯ç¬¬ä¸€ç¯‡æ–‡ç« çš„å†…å®¹</p>
            <span class="date">2024-01-01</span>
        </div>
        <div class="article" id="article-2">
            <h2>æ–‡ç« æ ‡é¢˜2</h2>
            <p class="author">ä½œè€…: æå››</p>
            <p class="content">è¿™æ˜¯ç¬¬äºŒç¯‡æ–‡ç« çš„å†…å®¹</p>
            <span class="date">2024-01-02</span>
        </div>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # find() - è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ 
    first_article = soup.find('div', class_='article')
    print(f"âœ… find()è¿”å›ç¬¬ä¸€ä¸ª: {first_article.get('id')}")
    
    # find_all() - è¿”å›æ‰€æœ‰åŒ¹é…çš„å…ƒç´ åˆ—è¡¨
    all_articles = soup.find_all('div', class_='article')
    print(f"âœ… find_all()è¿”å›æ‰€æœ‰: {len(all_articles)}ä¸ª")
    
    # é™åˆ¶è¿”å›æ•°é‡
    limited = soup.find_all('p', limit=2)
    print(f"âœ… é™åˆ¶æ•°é‡(limit=2): {len(limited)}ä¸ª")
    
    # å¤šä¸ªæ¡ä»¶
    author_p = soup.find('p', class_='author')
    print(f"âœ… å¤šæ¡ä»¶æŸ¥æ‰¾: {author_p.string}")
    
    # ä½¿ç”¨attrså‚æ•°ï¼ˆå½“å±æ€§æ˜¯Pythonå…³é”®å­—æ—¶ï¼‰
    article1 = soup.find('div', attrs={'id': 'article-1'})
    print(f"âœ… attrså‚æ•°: {article1.h2.string}")
    
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
    import re
    date_spans = soup.find_all('span', class_=re.compile('date'))
    print(f"âœ… æ­£åˆ™åŒ¹é…: {len(date_spans)}ä¸ªæ—¥æœŸ")
    
    # å‡½æ•°åŒ¹é…ï¼ˆé«˜çº§ï¼‰
    def has_author_class(tag):
        return tag.has_attr('class') and 'author' in tag['class']
    
    authors = soup.find_all(has_author_class)
    print(f"âœ… å‡½æ•°åŒ¹é…: {len(authors)}ä¸ªä½œè€…")
    
    print()


# ==================== 4. CSSé€‰æ‹©å™¨ ====================

def css_selector():
    """
    CSSé€‰æ‹©å™¨ï¼ˆæ¨èä½¿ç”¨ï¼Œæ›´ç®€æ´ï¼‰
    """
    print("=" * 60)
    print("4. CSSé€‰æ‹©å™¨")
    print("=" * 60)
    
    html = """
    <div class="container">
        <ul class="news-list">
            <li class="news-item active">
                <a href="/news/1">æ–°é—»æ ‡é¢˜1</a>
                <span class="category">ç§‘æŠ€</span>
            </li>
            <li class="news-item">
                <a href="/news/2">æ–°é—»æ ‡é¢˜2</a>
                <span class="category">è´¢ç»</span>
            </li>
            <li class="news-item">
                <a href="/news/3">æ–°é—»æ ‡é¢˜3</a>
                <span class="category">ä½“è‚²</span>
            </li>
        </ul>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # 1. æ ‡ç­¾é€‰æ‹©å™¨
    items = soup.select('li')
    print(f"âœ… æ ‡ç­¾é€‰æ‹©å™¨ 'li': {len(items)}ä¸ª")
    
    # 2. classé€‰æ‹©å™¨ï¼ˆ.ç±»åï¼‰
    news_items = soup.select('.news-item')
    print(f"âœ… Classé€‰æ‹©å™¨ '.news-item': {len(news_items)}ä¸ª")
    
    # 3. idé€‰æ‹©å™¨ï¼ˆ#idåï¼‰
    # container = soup.select('#container')
    
    # 4. åä»£é€‰æ‹©å™¨ï¼ˆç©ºæ ¼ï¼‰
    links = soup.select('.news-list a')
    print(f"âœ… åä»£é€‰æ‹©å™¨ '.news-list a': {len(links)}ä¸ª")
    for link in links:
        print(f"   - {link.string}: {link.get('href')}")
    
    # 5. å­é€‰æ‹©å™¨ï¼ˆ>ï¼‰
    direct_children = soup.select('.container > ul')
    print(f"âœ… å­é€‰æ‹©å™¨ '.container > ul': {len(direct_children)}ä¸ª")
    
    # 6. å¤šä¸ªç±»ï¼ˆ.class1.class2ï¼‰
    active_item = soup.select('.news-item.active')
    print(f"âœ… å¤šç±»é€‰æ‹©å™¨ '.news-item.active': {len(active_item)}ä¸ª")
    
    # 7. å±æ€§é€‰æ‹©å™¨
    tech_category = soup.select('span.category')
    print(f"âœ… å±æ€§é€‰æ‹©å™¨: {len(tech_category)}ä¸ªåˆ†ç±»")
    
    # 8. ç¬¬nä¸ªå…ƒç´ 
    first_item = soup.select('.news-item:nth-of-type(1)')
    print(f"âœ… ç¬¬1ä¸ªå…ƒç´ : {first_item[0].a.string}")
    
    print("""
ğŸ’¡ CSSé€‰æ‹©å™¨è¯­æ³•æ€»ç»“ï¼š
- æ ‡ç­¾: 'div'
- Class: '.classname'
- ID: '#idname'
- åä»£: '.parent .child'
- å­å…ƒç´ : '.parent > .child'
- å±æ€§: '[href]', '[href="/news"]'
- ç¬¬nä¸ª: ':nth-of-type(n)'
    """)
    
    print()


# ==================== 5. éå†DOMæ ‘ ====================

def navigate_tree():
    """
    éå†DOMæ ‘ï¼ˆçˆ¶èŠ‚ç‚¹ã€å­èŠ‚ç‚¹ã€å…„å¼ŸèŠ‚ç‚¹ï¼‰
    """
    print("=" * 60)
    print("5. éå†DOMæ ‘")
    print("=" * 60)
    
    html = """
    <div class="article">
        <h2>æ ‡é¢˜</h2>
        <p class="author">ä½œè€…</p>
        <p class="content">å†…å®¹</p>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # è·å–pæ ‡ç­¾
    p_tag = soup.find('p', class_='author')
    
    # 1. çˆ¶èŠ‚ç‚¹
    parent = p_tag.parent
    print(f"âœ… çˆ¶èŠ‚ç‚¹: {parent.name}")
    
    # 2. æ‰€æœ‰çˆ¶èŠ‚ç‚¹
    parents = [p.name for p in p_tag.parents]
    print(f"âœ… æ‰€æœ‰çˆ¶èŠ‚ç‚¹: {parents}")
    
    # 3. å­èŠ‚ç‚¹
    div_tag = soup.find('div')
    children = list(div_tag.children)
    print(f"âœ… å­èŠ‚ç‚¹æ•°: {len(children)}")
    for child in div_tag.children:
        if child.name:  # è·³è¿‡ç©ºç™½æ–‡æœ¬
            print(f"   - {child.name}: {child.string}")
    
    # 4. ä¸‹ä¸€ä¸ªå…„å¼ŸèŠ‚ç‚¹
    next_sibling = p_tag.find_next_sibling()
    print(f"âœ… ä¸‹ä¸€ä¸ªå…„å¼Ÿ: {next_sibling.get('class')}")
    
    # 5. ä¸Šä¸€ä¸ªå…„å¼ŸèŠ‚ç‚¹
    prev_sibling = p_tag.find_previous_sibling()
    print(f"âœ… ä¸Šä¸€ä¸ªå…„å¼Ÿ: {prev_sibling.name}")
    
    print()


# ==================== 6. æå–æ•°æ®æŠ€å·§ ====================

def extract_data():
    """
    æå–æ•°æ®çš„å„ç§æŠ€å·§
    """
    print("=" * 60)
    print("6. æå–æ•°æ®æŠ€å·§")
    print("=" * 60)
    
    html = """
    <div class="product">
        <h3 class="title">iPhone 15 Pro</h3>
        <span class="price" data-value="7999">Â¥7999</span>
        <img src="/images/iphone.jpg" alt="iPhoneå›¾ç‰‡">
        <p class="desc">
            è¿™æ˜¯ä¸€æ¬¾<strong>é«˜ç«¯</strong>æ‰‹æœº
            æ”¯æŒ5Gç½‘ç»œ
        </p>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # 1. è·å–æ–‡æœ¬å†…å®¹
    title = soup.find('h3', class_='title')
    print(f"âœ… .string: {title.string}")  # è·å–ç›´æ¥æ–‡æœ¬
    print(f"âœ… .get_text(): {title.get_text()}")  # è·å–æ‰€æœ‰æ–‡æœ¬
    
    # 2. è·å–å±æ€§å€¼
    price = soup.find('span', class_='price')
    print(f"âœ… data-valueå±æ€§: {price.get('data-value')}")
    print(f"âœ… æˆ–ä½¿ç”¨['data-value']: {price['data-value']}")
    
    # 3. è·å–å›¾ç‰‡é“¾æ¥
    img = soup.find('img')
    print(f"âœ… src: {img.get('src')}")
    print(f"âœ… alt: {img.get('alt')}")
    
    # 4. è·å–åŒ…å«å­æ ‡ç­¾çš„æ–‡æœ¬
    desc = soup.find('p', class_='desc')
    print(f"âœ… å®Œæ•´æ–‡æœ¬: {desc.get_text(strip=True)}")  # strip=Trueå»é™¤ç©ºç™½
    
    # 5. åˆ†éš”ç¬¦å¤„ç†
    print(f"âœ… åˆ†éš”ç¬¦å¤„ç†: {desc.get_text(separator=' ', strip=True)}")
    
    # 6. æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å­˜åœ¨
    rating = soup.find('span', class_='rating')
    if rating:
        print(f"âœ… è¯„åˆ†: {rating.string}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°è¯„åˆ†ä¿¡æ¯")
    
    # 7. ä½¿ç”¨get()å®‰å…¨è·å–å±æ€§ï¼ˆä¸å­˜åœ¨æ—¶è¿”å›Noneï¼‰
    link = soup.find('a')
    href = link.get('href') if link else None
    print(f"âœ… å®‰å…¨è·å–: {href}")
    
    print()


# ==================== 7. å®æˆ˜æ¡ˆä¾‹ï¼šè§£ææ–°é—»åˆ—è¡¨ ====================

def parse_news_example():
    """
    å®æˆ˜ï¼šè§£ææ–°é—»åˆ—è¡¨é¡µé¢
    """
    print("=" * 60)
    print("7. å®æˆ˜æ¡ˆä¾‹ï¼šè§£ææ–°é—»åˆ—è¡¨")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿæ–°é—»åˆ—è¡¨HTML
    html = """
    <div class="news-container">
        <div class="news-item">
            <a href="/news/1" class="title">Python 3.12æ­£å¼å‘å¸ƒ</a>
            <span class="author">ä½œè€…: å¼ ä¸‰</span>
            <span class="date">2024-01-15</span>
            <p class="summary">Python 3.12å¸¦æ¥äº†æ€§èƒ½æå‡...</p>
            <span class="views">æµè§ˆ: 1000</span>
        </div>
        <div class="news-item">
            <a href="/news/2" class="title">AIæŠ€æœ¯çš„æœ€æ–°è¿›å±•</a>
            <span class="author">ä½œè€…: æå››</span>
            <span class="date">2024-01-16</span>
            <p class="summary">äººå·¥æ™ºèƒ½é¢†åŸŸè¿æ¥é‡å¤§çªç ´...</p>
            <span class="views">æµè§ˆ: 2000</span>
        </div>
        <div class="news-item">
            <a href="/news/3" class="title">Webå¼€å‘è¶‹åŠ¿2024</a>
            <span class="author">ä½œè€…: ç‹äº”</span>
            <span class="date">2024-01-17</span>
            <p class="summary">2024å¹´Webå¼€å‘çš„æ–°è¶‹åŠ¿...</p>
            <span class="views">æµè§ˆ: 1500</span>
        </div>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # è§£ææ‰€æœ‰æ–°é—»
    news_list = []
    news_items = soup.find_all('div', class_='news-item')
    
    for item in news_items:
        # æå–å„ä¸ªå­—æ®µ
        title_tag = item.find('a', class_='title')
        title = title_tag.string
        url = title_tag.get('href')
        
        author = item.find('span', class_='author').string.replace('ä½œè€…: ', '')
        date = item.find('span', class_='date').string
        summary = item.find('p', class_='summary').string
        views = item.find('span', class_='views').string.replace('æµè§ˆ: ', '')
        
        # æ„é€ å­—å…¸
        news = {
            'title': title,
            'url': url,
            'author': author,
            'date': date,
            'summary': summary,
            'views': int(views)
        }
        
        news_list.append(news)
    
    # æ‰“å°ç»“æœ
    print(f"âœ… å…±è§£æ {len(news_list)} æ¡æ–°é—»\n")
    for i, news in enumerate(news_list, 1):
        print(f"æ–°é—» {i}:")
        print(f"  æ ‡é¢˜: {news['title']}")
        print(f"  é“¾æ¥: {news['url']}")
        print(f"  ä½œè€…: {news['author']}")
        print(f"  æ—¥æœŸ: {news['date']}")
        print(f"  æµè§ˆ: {news['views']}")
        print()


# ==================== 8. å®æˆ˜ï¼šçˆ¬å–çœŸå®ç½‘é¡µ ====================

def crawl_real_website():
    """
    çˆ¬å–çœŸå®ç½‘ç«™ç¤ºä¾‹ï¼ˆä½¿ç”¨å…¬å¼€APIï¼‰
    """
    print("=" * 60)
    print("8. çˆ¬å–çœŸå®ç½‘é¡µç¤ºä¾‹")
    print("=" * 60)
    
    try:
        # çˆ¬å–ç¤ºä¾‹ç½‘ç«™ï¼ˆhttp://books.toscrape.com æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºçˆ¬è™«ç»ƒä¹ çš„ç½‘ç«™ï¼‰
        url = "http://books.toscrape.com/"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è§£æä¹¦ç±åˆ—è¡¨
        books = soup.select('.product_pod')
        print(f"âœ… æ‰¾åˆ° {len(books)} æœ¬ä¹¦\n")
        
        for i, book in enumerate(books[:5], 1):  # åªæ˜¾ç¤ºå‰5æœ¬
            # æ ‡é¢˜
            title = book.select_one('h3 a').get('title')
            
            # ä»·æ ¼
            price = book.select_one('.price_color').string
            
            # è¯„åˆ†
            rating_class = book.select_one('.star-rating').get('class')[1]
            
            # åº“å­˜
            availability = book.select_one('.availability').string.strip()
            
            print(f"ä¹¦ç± {i}:")
            print(f"  æ ‡é¢˜: {title}")
            print(f"  ä»·æ ¼: {price}")
            print(f"  è¯„åˆ†: {rating_class}")
            print(f"  åº“å­˜: {availability}")
            print()
        
        print("âœ… çˆ¬å–æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")


# ==================== ç»ƒä¹ é¢˜ ====================

def exercises():
    """
    è¯¾åç»ƒä¹ é¢˜
    """
    print("=" * 60)
    print("ğŸ“ è¯¾åç»ƒä¹ ")
    print("=" * 60)
    
    print("""
è¯·å®Œæˆä»¥ä¸‹ç»ƒä¹ ï¼š

ã€ç»ƒä¹ 1ã€‘è§£æç”µå•†å•†å“
ç»™å®šä¸€ä¸ªå•†å“åˆ—è¡¨HTMLï¼Œæå–ï¼š
- å•†å“åç§°
- ä»·æ ¼
- è¯„åˆ†
- è¯„è®ºæ•°
- å•†å“å›¾ç‰‡é“¾æ¥

ã€ç»ƒä¹ 2ã€‘è¡¨æ ¼æ•°æ®æå–
è§£æä¸€ä¸ªHTMLè¡¨æ ¼ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼š
[
    ['å§“å', 'å¹´é¾„', 'åŸå¸‚'],
    ['å¼ ä¸‰', 25, 'åŒ—äº¬'],
    ['æå››', 30, 'ä¸Šæµ·']
]

ã€ç»ƒä¹ 3ã€‘é“¾æ¥çˆ¬è™«
çˆ¬å–ä¸€ä¸ªç½‘é¡µçš„æ‰€æœ‰é“¾æ¥ï¼Œåˆ†ç±»ä¸ºï¼š
- ç«™å†…é“¾æ¥ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
- ç«™å¤–é“¾æ¥ï¼ˆç»å¯¹è·¯å¾„ï¼‰
- å›¾ç‰‡é“¾æ¥
- æ–‡æ¡£é“¾æ¥ï¼ˆpdf, docç­‰ï¼‰

ã€ç»ƒä¹ 4ã€‘ç»¼åˆåº”ç”¨
çˆ¬å– http://books.toscrape.com/ çš„å¤šä¸ªåˆ†ç±»
- éå†æ‰€æœ‰åˆ†ç±»
- æ¯ä¸ªåˆ†ç±»çˆ¬å–å‰10æœ¬ä¹¦
- ä¿å­˜ä¸ºJSONæ–‡ä»¶

æç¤ºï¼šä½¿ç”¨CSSé€‰æ‹©å™¨ä¼šæ›´ç®€æ´ï¼
    """)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "=" * 60)
    print("BeautifulSoupç½‘é¡µè§£ææ•™ç¨‹")
    print("=" * 60 + "\n")
    
    html_basics()
    bs4_basics()
    find_methods()
    css_selector()
    navigate_tree()
    extract_data()
    parse_news_example()
    crawl_real_website()
    exercises()
    
    print("=" * 60)
    print("âœ… BeautifulSoupå­¦ä¹ å®Œæˆï¼")
    print("ğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("   1. ä½¿ç”¨find()å’Œfind_all()æŸ¥æ‰¾å…ƒç´ ")
    print("   2. CSSé€‰æ‹©å™¨æ›´ç®€æ´ï¼ˆæ¨èï¼‰")
    print("   3. æå–æ–‡æœ¬ç”¨.stringæˆ–.get_text()")
    print("   4. æå–å±æ€§ç”¨.get()æˆ–['attr']")
    print("â­ï¸  ä¸‹ä¸€æ­¥ï¼šå­¦ä¹  02_xpath.py")
    print("=" * 60)


if __name__ == "__main__":
    main()

