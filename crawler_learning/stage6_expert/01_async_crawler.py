"""
ç¬¬å…­é˜¶æ®µ - å¼‚æ­¥çˆ¬è™«

ä½¿ç”¨asyncioå’Œaiohttpå®ç°é«˜æ€§èƒ½å¼‚æ­¥çˆ¬è™«
"""

import asyncio
import time
from typing import List

# ==================== 1. å¼‚æ­¥ç¼–ç¨‹åŸºç¡€ ====================

def async_basics():
    """
    å¼‚æ­¥ç¼–ç¨‹åŸºç¡€æ¦‚å¿µ
    """
    print("=" * 60)
    print("1. å¼‚æ­¥ç¼–ç¨‹åŸºç¡€")
    print("=" * 60 + "\n")
    
    print("""
ã€ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿã€‘

åŒæ­¥ï¼ˆSynchronousï¼‰ï¼š
- ä¸€ä¸ªä»»åŠ¡æ‰§è¡Œå®Œæ‰èƒ½æ‰§è¡Œä¸‹ä¸€ä¸ª
- ä»»åŠ¡ç­‰å¾…æ—¶CPUç©ºé—²
- é€‚åˆCPUå¯†é›†å‹ä»»åŠ¡

å¼‚æ­¥ï¼ˆAsynchronousï¼‰ï¼š
- ä»»åŠ¡å¯ä»¥å¹¶å‘æ‰§è¡Œ
- ç­‰å¾…æ—¶å¯ä»¥æ‰§è¡Œå…¶ä»–ä»»åŠ¡
- é€‚åˆIOå¯†é›†å‹ä»»åŠ¡ï¼ˆçˆ¬è™«ï¼ï¼‰

ã€æ ¸å¿ƒæ¦‚å¿µã€‘

1. åç¨‹ (Coroutine)
   - ç”¨async defå®šä¹‰çš„å‡½æ•°
   - å¯ä»¥æš‚åœå’Œæ¢å¤æ‰§è¡Œ
   
2. awaitå…³é”®å­—
   - ç­‰å¾…åç¨‹å®Œæˆ
   - é‡Šæ”¾æ§åˆ¶æƒç»™äº‹ä»¶å¾ªç¯
   
3. äº‹ä»¶å¾ªç¯ (Event Loop)
   - ç®¡ç†å’Œè°ƒåº¦åç¨‹
   - asyncio.run()

ã€ç¤ºä¾‹å¯¹æ¯”ã€‘

# åŒæ­¥ä»£ç 
def fetch_sync(url):
    response = requests.get(url)  # é˜»å¡
    return response.text

for url in urls:
    result = fetch_sync(url)
    # æ¯æ¬¡éƒ½è¦ç­‰å¾…

# å¼‚æ­¥ä»£ç 
async def fetch_async(session, url):
    async with session.get(url) as response:  # ä¸é˜»å¡
        return await response.text()

async def main():
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_async(session, url) for url in urls]
        results = await asyncio.gather(*tasks)  # å¹¶å‘æ‰§è¡Œ

asyncio.run(main())

ã€æ€§èƒ½å·®å¼‚ã€‘

å‡è®¾æ¯ä¸ªè¯·æ±‚éœ€è¦1ç§’ï¼š

åŒæ­¥æ–¹å¼ï¼š
- 10ä¸ªè¯·æ±‚ = 10ç§’ï¼ˆä¸²è¡Œï¼‰

å¼‚æ­¥æ–¹å¼ï¼š
- 10ä¸ªè¯·æ±‚ â‰ˆ 1ç§’ï¼ˆå¹¶è¡Œï¼‰

ã€ä½•æ—¶ä½¿ç”¨å¼‚æ­¥ï¼Ÿã€‘

âœ… é€‚åˆï¼š
- å¤§é‡ç½‘ç»œè¯·æ±‚
- IOå¯†é›†å‹ä»»åŠ¡
- éœ€è¦é«˜å¹¶å‘

âŒ ä¸é€‚åˆï¼š
- CPUå¯†é›†å‹è®¡ç®—
- ç®€å•çš„å‡ ä¸ªè¯·æ±‚
- ä»£ç å¤æ‚åº¦å¢åŠ 
    """)


# ==================== 2. aiohttpåŸºç¡€ä½¿ç”¨ ====================

def aiohttp_basics():
    """
    aiohttpåŸºç¡€ä½¿ç”¨
    """
    print("\n" + "=" * 60)
    print("2. aiohttpåŸºç¡€ä½¿ç”¨")
    print("=" * 60 + "\n")
    
    print("""
aiohttpæ˜¯å¼‚æ­¥HTTPå®¢æˆ·ç«¯åº“

ã€å®‰è£…ã€‘
pip install aiohttp

ã€åŸºæœ¬ä½¿ç”¨ã€‘
    """)
    
    example = '''
import aiohttp
import asyncio

async def fetch(url):
    """è·å–å•ä¸ªURL"""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

# è¿è¡Œ
result = asyncio.run(fetch("https://www.python.org"))

ã€å…³é”®ç‚¹ã€‘

1. ClientSession
   - ç®¡ç†è¿æ¥æ± 
   - å¤ç”¨è¿æ¥
   - å¿…é¡»åœ¨asyncä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨
   
2. async with
   - å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
   - è‡ªåŠ¨å¤„ç†èµ„æºé‡Šæ”¾
   
3. await response.text()
   - ç­‰å¾…å“åº”ä½“
   - è¿˜æœ‰ï¼šresponse.json(), response.read()

ã€å®Œæ•´ç¤ºä¾‹ã€‘

async def fetch_page(session, url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        async with session.get(url, timeout=10) as response:
            if response.status == 200:
                return await response.text()
            else:
                print(f"é”™è¯¯çŠ¶æ€ç : {response.status}")
                return None
    except asyncio.TimeoutError:
        print(f"è¯·æ±‚è¶…æ—¶: {url}")
        return None
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

async def main():
    urls = [
        "https://www.python.org",
        "https://github.com",
        "https://stackoverflow.com",
    ]
    
    # åˆ›å»ºSessionï¼ˆå¤ç”¨è¿æ¥ï¼‰
    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [fetch_page(session, url) for url in urls]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks)
        
        return results

# è¿è¡Œ
results = asyncio.run(main())
    '''
    
    print(example)


# ==================== 3. å¹¶å‘æ§åˆ¶ ====================

def concurrency_control():
    """
    å¹¶å‘æ§åˆ¶
    """
    print("\n" + "=" * 60)
    print("3. å¹¶å‘æ§åˆ¶")
    print("=" * 60 + "\n")
    
    print("""
ã€ä¸ºä»€ä¹ˆéœ€è¦æ§åˆ¶å¹¶å‘ï¼Ÿã€‘
- é¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
- é¿å…è¢«å°IP
- é¿å…å†…å­˜æº¢å‡º

ã€æ–¹æ³•1ï¼šä½¿ç”¨Semaphoreï¼ˆä¿¡å·é‡ï¼‰ã€‘
    """)
    
    example1 = '''
async def fetch_with_semaphore(session, url, semaphore):
    """å¸¦ä¿¡å·é‡çš„è¯·æ±‚"""
    async with semaphore:  # è·å–ä¿¡å·é‡
        return await fetch_page(session, url)

async def main():
    urls = ["url1", "url2", "url3", ...]  # å¾ˆå¤šURL
    
    # é™åˆ¶å¹¶å‘æ•°ä¸º10
    semaphore = asyncio.Semaphore(10)
    
    async with aiohttp.ClientSession() as session:
        tasks = [
            fetch_with_semaphore(session, url, semaphore)
            for url in urls
        ]
        results = await asyncio.gather(*tasks)
    '''
    
    print(example1)
    
    print("\nã€æ–¹æ³•2ï¼šåˆ†æ‰¹å¤„ç†ã€‘\n")
    
    example2 = '''
async def fetch_batch(session, urls, batch_size=10):
    """åˆ†æ‰¹çˆ¬å–"""
    results = []
    
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i+batch_size]
        
        # æ¯æ‰¹å¹¶å‘æ‰§è¡Œ
        tasks = [fetch_page(session, url) for url in batch]
        batch_results = await asyncio.gather(*tasks)
        
        results.extend(batch_results)
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        await asyncio.sleep(1)
    
    return results
    '''
    
    print(example2)


# ==================== 4. å¼‚æ­¥çˆ¬è™«ç±» ====================

async_crawler_example = '''
ã€å®Œæ•´å¼‚æ­¥çˆ¬è™«ç±»ã€‘

import aiohttp
import asyncio
from typing import List, Dict
import time

class AsyncCrawler:
    """å¼‚æ­¥çˆ¬è™«ç±»"""
    
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.results = []
        
        # é…ç½®
        self.headers = {
            "User-Agent": "Mozilla/5.0 ..."
        }
    
    async def fetch(self, session, url):
        """è·å–å•ä¸ªURL"""
        async with self.semaphore:
            try:
                async with session.get(
                    url, 
                    headers=self.headers,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    return {
                        "url": url,
                        "status": response.status,
                        "content": await response.text()
                    }
            except Exception as e:
                return {
                    "url": url,
                    "status": 0,
                    "error": str(e)
                }
    
    async def crawl(self, urls: List[str]):
        """çˆ¬å–å¤šä¸ªURL"""
        print(f"å¼€å§‹çˆ¬å– {len(urls)} ä¸ªURL...")
        start_time = time.time()
        
        # åˆ›å»ºè¿æ¥å™¨ï¼ˆå¤ç”¨è¿æ¥ï¼‰
        connector = aiohttp.TCPConnector(limit=100)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # åˆ›å»ºä»»åŠ¡
            tasks = [self.fetch(session, url) for url in urls]
            
            # å¹¶å‘æ‰§è¡Œ
            self.results = await asyncio.gather(*tasks)
        
        elapsed = time.time() - start_time
        print(f"çˆ¬å–å®Œæˆï¼è€—æ—¶: {elapsed:.2f}ç§’")
        
        return self.results
    
    def run(self, urls: List[str]):
        """è¿è¡Œçˆ¬è™«"""
        return asyncio.run(self.crawl(urls))

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    urls = [
        "https://www.python.org",
        "https://github.com",
        "https://stackoverflow.com",
        # ... æ›´å¤šURL
    ]
    
    crawler = AsyncCrawler(max_concurrent=20)
    results = crawler.run(urls)
    
    # å¤„ç†ç»“æœ
    for result in results:
        print(f"{result['url']}: {result['status']}")
'''


def async_crawler_class():
    """
    å¼‚æ­¥çˆ¬è™«ç±»ç¤ºä¾‹
    """
    print("\n" + "=" * 60)
    print("4. å®Œæ•´å¼‚æ­¥çˆ¬è™«ç±»")
    print("=" * 60 + "\n")
    
    print(async_crawler_example)


# ==================== 5. æ€§èƒ½å¯¹æ¯” ====================

def performance_comparison():
    """
    æ€§èƒ½å¯¹æ¯”
    """
    print("\n" + "=" * 60)
    print("5. æ€§èƒ½å¯¹æ¯”")
    print("=" * 60 + "\n")
    
    comparison = '''
ã€å®æµ‹å¯¹æ¯”ã€‘

æµ‹è¯•ï¼šçˆ¬å–100ä¸ªURL

1. requestsåŒæ­¥
   è€—æ—¶: ~120ç§’
   CPU: 5%
   å†…å­˜: 50MB
   
2. requests + å¤šçº¿ç¨‹(10çº¿ç¨‹)
   è€—æ—¶: ~15ç§’
   CPU: 20%
   å†…å­˜: 80MB
   
3. asyncio + aiohttp
   è€—æ—¶: ~3ç§’
   CPU: 15%
   å†…å­˜: 60MB

ã€ç»“è®ºã€‘
- å¼‚æ­¥æ–¹å¼æœ€å¿«ï¼ˆ40å€æå‡ï¼‰
- èµ„æºå ç”¨åˆç†
- ä»£ç å¤æ‚åº¦å¢åŠ 

ã€é€‚ç”¨åœºæ™¯ã€‘

URLæ•°é‡       æ¨èæ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
< 10         requestsåŒæ­¥
10-100       requestså¤šçº¿ç¨‹
> 100        asyncioå¼‚æ­¥
> 10000      Scrapy
    '''
    
    print(comparison)


# ==================== 6. å¸¸è§é—®é¢˜ ====================

def common_issues():
    """
    å¸¸è§é—®é¢˜
    """
    print("\n" + "=" * 60)
    print("6. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ")
    print("=" * 60 + "\n")
    
    print("""
ã€é—®é¢˜1ï¼šè¿æ¥æ•°è¿‡å¤šã€‘
é”™è¯¯: Too many open files

è§£å†³ï¼š
1. é™åˆ¶å¹¶å‘æ•°
   semaphore = asyncio.Semaphore(50)

2. é™åˆ¶è¿æ¥æ± 
   connector = aiohttp.TCPConnector(limit=100)

ã€é—®é¢˜2ï¼šå†…å­˜å ç”¨å¤§ã€‘
åŸå› ï¼šä¸€æ¬¡æ€§åˆ›å»ºå¤ªå¤šä»»åŠ¡

è§£å†³ï¼š
1. åˆ†æ‰¹å¤„ç†
2. ä½¿ç”¨é˜Ÿåˆ—
3. åŠæ—¶é‡Šæ”¾èµ„æº

ã€é—®é¢˜3ï¼šæŸäº›è¯·æ±‚å¡ä½ã€‘
åŸå› ï¼šæ²¡æœ‰è®¾ç½®è¶…æ—¶

è§£å†³ï¼š
timeout = aiohttp.ClientTimeout(total=10)
async with session.get(url, timeout=timeout) as response:
    ...

ã€é—®é¢˜4ï¼šå¦‚ä½•ä¿æŒCookieã€‘
è§£å†³ï¼šä½¿ç”¨åŒä¸€ä¸ªSession
async with aiohttp.ClientSession() as session:
    # ç™»å½•
    await session.post(login_url, data=credentials)
    
    # åç»­è¯·æ±‚è‡ªåŠ¨å¸¦Cookie
    await session.get(protected_url)

ã€é—®é¢˜5ï¼šå¦‚ä½•æ·»åŠ é‡è¯•ã€‘
è§£å†³ï¼šä½¿ç”¨aiohttp-retryåº“
from aiohttp_retry import RetryClient, ExponentialRetry

retry_options = ExponentialRetry(attempts=3)
retry_client = RetryClient(raise_for_status=False, retry_options=retry_options)

async with retry_client.get(url) as response:
    ...
    """)


# ==================== ç»ƒä¹ é¢˜ ====================

def exercises():
    """
    è¯¾åç»ƒä¹ 
    """
    print("\n" + "=" * 60)
    print("ğŸ“ è¯¾åç»ƒä¹ ")
    print("=" * 60 + "\n")
    
    print("""
ã€ç»ƒä¹ 1ã€‘åŸºç¡€å¼‚æ­¥
ç¼–å†™å¼‚æ­¥å‡½æ•°ï¼š
1. è·å–10ä¸ªURLçš„å†…å®¹
2. è®¡ç®—æ€»è€—æ—¶
3. å¯¹æ¯”åŒæ­¥æ–¹å¼çš„è€—æ—¶

ã€ç»ƒä¹ 2ã€‘å¹¶å‘æ§åˆ¶
å®ç°å¹¶å‘çˆ¬è™«ï¼š
1. é™åˆ¶æœ€å¤š20ä¸ªå¹¶å‘
2. æ·»åŠ è¿›åº¦æ˜¾ç¤º
3. ç»Ÿè®¡æˆåŠŸ/å¤±è´¥æ•°é‡

ã€ç»ƒä¹ 3ã€‘å¼‚æ­¥è§£æ
ç»“åˆBeautifulSoupï¼š
1. å¼‚æ­¥è·å–é¡µé¢
2. åŒæ­¥è§£æHTML
3. æå–æ•°æ®
4. ä¿å­˜ç»“æœ

ã€ç»ƒä¹ 4ã€‘ç”Ÿäº§è€…-æ¶ˆè´¹è€…
å®ç°é˜Ÿåˆ—æ¨¡å¼ï¼š
1. ç”Ÿäº§è€…ï¼šç”ŸæˆURL
2. æ¶ˆè´¹è€…ï¼šå¹¶å‘çˆ¬å–
3. ä½¿ç”¨asyncio.Queue

ã€ç»ƒä¹ 5ã€‘å®Œæ•´é¡¹ç›®
å¼‚æ­¥æ–°é—»çˆ¬è™«ï¼š
1. çˆ¬å–å¤šä¸ªæ–°é—»ç½‘ç«™
2. é™åˆ¶å¹¶å‘æ•°
3. å¼‚å¸¸é‡è¯•
4. æ•°æ®å­˜å‚¨
5. æ€§èƒ½ç»Ÿè®¡

æç¤ºï¼š
- å…ˆä»å°è§„æ¨¡æµ‹è¯•å¼€å§‹
- é€æ­¥å¢åŠ å¹¶å‘æ•°
- æ³¨æ„èµ„æºé‡Šæ”¾
- æ·»åŠ æ—¥å¿—è®°å½•
    """)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "=" * 60)
    print("ç¬¬å…­é˜¶æ®µ - å¼‚æ­¥çˆ¬è™«")
    print("=" * 60)
    
    async_basics()
    aiohttp_basics()
    concurrency_control()
    async_crawler_class()
    performance_comparison()
    common_issues()
    exercises()
    
    print("\n" + "=" * 60)
    print("âœ… å¼‚æ­¥çˆ¬è™«å­¦ä¹ å®Œæˆï¼")
    print("ğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("   1. async/awaitè¯­æ³•")
    print("   2. aiohttpæ›¿ä»£requests")
    print("   3. asyncio.gather()å¹¶å‘æ‰§è¡Œ")
    print("   4. Semaphoreæ§åˆ¶å¹¶å‘æ•°")
    print("   5. æ€§èƒ½æå‡æ˜¾è‘—ï¼ˆ10-100å€ï¼‰")
    print("â­ï¸  ä¸‹ä¸€æ­¥ï¼šå­¦ä¹  02_proxy_pool.py")
    print("=" * 60)


if __name__ == "__main__":
    main()

