"""
ç¬¬ä¸‰é˜¶æ®µå®æˆ˜ - æ–°é—»çˆ¬è™«

é¡¹ç›®ï¼šçˆ¬å–æ–°é—»ç½‘ç«™çš„æ–‡ç« åˆ—è¡¨å’Œè¯¦æƒ…
æŠ€æœ¯ï¼šrequests + BeautifulSoup + JSONå­˜å‚¨
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
import os

# ==================== é…ç½®ç±» ====================

class Config:
    """çˆ¬è™«é…ç½®"""
    
    # è¯·æ±‚é…ç½®
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    
    TIMEOUT = 10  # è¯·æ±‚è¶…æ—¶æ—¶é—´
    DELAY = 1  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    
    # å­˜å‚¨é…ç½®
    OUTPUT_DIR = "news_data"
    OUTPUT_FILE = "news_articles.json"


# ==================== æ–°é—»çˆ¬è™«ç±» ====================

class NewsCrawler:
    """
    æ–°é—»çˆ¬è™«ç±»
    
    åŠŸèƒ½ï¼š
    1. çˆ¬å–æ–°é—»åˆ—è¡¨
    2. çˆ¬å–æ–°é—»è¯¦æƒ…
    3. æ•°æ®æ¸…æ´—
    4. ä¿å­˜æ•°æ®
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(Config.HEADERS)
        self.articles = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
    
    def fetch_page(self, url: str) -> Optional[str]:
        """
        è·å–ç½‘é¡µå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰
        
        Args:
            url: ç›®æ ‡URL
            
        Returns:
            ç½‘é¡µHTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        for attempt in range(Config.MAX_RETRIES):
            try:
                response = self.session.get(url, timeout=Config.TIMEOUT)
                response.raise_for_status()
                response.encoding = response.apparent_encoding  # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                return response.text
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡ï¼‰: {e}")
                if attempt < Config.MAX_RETRIES - 1:
                    time.sleep(2)
                else:
                    return None
    
    def parse_list_page(self, html: str, base_url: str) -> List[Dict]:
        """
        è§£æåˆ—è¡¨é¡µ
        
        Args:
            html: åˆ—è¡¨é¡µHTML
            base_url: åŸºç¡€URLï¼ˆç”¨äºæ‹¼æ¥ç›¸å¯¹è·¯å¾„ï¼‰
            
        Returns:
            æ–‡ç« åˆ—è¡¨ï¼ˆåŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦ç­‰ï¼‰
        """
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # è¿™é‡Œä»¥ç¤ºä¾‹ç½‘ç«™ä¸ºä¾‹ï¼ˆå®é™…é¡¹ç›®ä¸­æ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        # å‡è®¾æ–°é—»åˆ—è¡¨åœ¨ class='news-item' çš„divä¸­
        news_items = soup.find_all('div', class_='news-item')
        
        for item in news_items:
            try:
                # æå–æ ‡é¢˜å’Œé“¾æ¥
                title_tag = item.find('a', class_='title')
                if not title_tag:
                    continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag.get('href', '')
                
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if link.startswith('/'):
                    link = base_url + link
                elif not link.startswith('http'):
                    link = base_url + '/' + link
                
                # æå–å…¶ä»–ä¿¡æ¯
                author_tag = item.find('span', class_='author')
                author = author_tag.get_text(strip=True) if author_tag else "æœªçŸ¥"
                
                date_tag = item.find('span', class_='date')
                pub_date = date_tag.get_text(strip=True) if date_tag else ""
                
                summary_tag = item.find('p', class_='summary')
                summary = summary_tag.get_text(strip=True) if summary_tag else ""
                
                article = {
                    'title': title,
                    'link': link,
                    'author': author,
                    'pub_date': pub_date,
                    'summary': summary,
                }
                
                articles.append(article)
                
            except Exception as e:
                print(f"âš ï¸ è§£ææ–‡ç« å¤±è´¥: {e}")
                continue
        
        return articles
    
    def parse_detail_page(self, html: str) -> Dict:
        """
        è§£æè¯¦æƒ…é¡µ
        
        Args:
            html: è¯¦æƒ…é¡µHTML
            
        Returns:
            æ–‡ç« è¯¦ç»†å†…å®¹
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # æå–æ­£æ–‡ï¼ˆæ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´ï¼‰
            content_tag = soup.find('div', class_='article-content')
            if content_tag:
                # åˆ é™¤scriptå’Œstyleæ ‡ç­¾
                for tag in content_tag.find_all(['script', 'style']):
                    tag.decompose()
                
                content = content_tag.get_text(separator='\n', strip=True)
            else:
                content = ""
            
            # æå–å›¾ç‰‡
            images = []
            img_tags = soup.find_all('img', class_='article-img')
            for img in img_tags:
                src = img.get('src', '')
                if src:
                    images.append(src)
            
            # æå–æ ‡ç­¾
            tags = []
            tag_elements = soup.find_all('a', class_='tag')
            for tag in tag_elements:
                tags.append(tag.get_text(strip=True))
            
            return {
                'content': content,
                'images': images,
                'tags': tags,
            }
            
        except Exception as e:
            print(f"âš ï¸ è§£æè¯¦æƒ…å¤±è´¥: {e}")
            return {
                'content': '',
                'images': [],
                'tags': [],
            }
    
    def crawl_article_detail(self, article: Dict) -> Dict:
        """
        çˆ¬å–æ–‡ç« è¯¦æƒ…
        
        Args:
            article: æ–‡ç« åŸºæœ¬ä¿¡æ¯
            
        Returns:
            å®Œæ•´çš„æ–‡ç« ä¿¡æ¯
        """
        print(f"ğŸ“– æ­£åœ¨çˆ¬å–: {article['title']}")
        
        html = self.fetch_page(article['link'])
        if not html:
            return article
        
        # è§£æè¯¦æƒ…
        detail = self.parse_detail_page(html)
        article.update(detail)
        
        # æ·»åŠ çˆ¬å–æ—¶é—´
        article['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
        time.sleep(Config.DELAY)
        
        return article
    
    def crawl(self, list_url: str, max_pages: int = 1, max_articles: int = 10):
        """
        æ‰§è¡Œçˆ¬å–ä»»åŠ¡
        
        Args:
            list_url: åˆ—è¡¨é¡µURL
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            max_articles: æ¯é¡µæœ€å¤§çˆ¬å–æ–‡ç« æ•°
        """
        print("=" * 60)
        print("ğŸš€ å¼€å§‹çˆ¬å–æ–°é—»")
        print("=" * 60)
        
        base_url = '/'.join(list_url.split('/')[:3])
        
        for page in range(1, max_pages + 1):
            print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # æ„é€ åˆ†é¡µURLï¼ˆæ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´ï¼‰
            if page == 1:
                url = list_url
            else:
                url = f"{list_url}?page={page}"
            
            # è·å–åˆ—è¡¨é¡µ
            html = self.fetch_page(url)
            if not html:
                print(f"âŒ è·å–ç¬¬ {page} é¡µå¤±è´¥")
                continue
            
            # è§£æåˆ—è¡¨é¡µ
            articles = self.parse_list_page(html, base_url)
            print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            # çˆ¬å–è¯¦æƒ…
            for i, article in enumerate(articles[:max_articles], 1):
                try:
                    full_article = self.crawl_article_detail(article)
                    self.articles.append(full_article)
                    print(f"âœ… [{i}/{len(articles[:max_articles])}] å®Œæˆ")
                except Exception as e:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                    continue
            
            time.sleep(Config.DELAY)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.articles)} ç¯‡æ–‡ç« ")
    
    def save_to_json(self, filename: Optional[str] = None):
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        """
        if not filename:
            filename = os.path.join(Config.OUTPUT_DIR, Config.OUTPUT_FILE)
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.articles, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def display_statistics(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"æ–‡ç« æ€»æ•°: {len(self.articles)}")
        
        # ç»Ÿè®¡ä½œè€…
        authors = {}
        for article in self.articles:
            author = article.get('author', 'æœªçŸ¥')
            authors[author] = authors.get(author, 0) + 1
        
        print(f"ä½œè€…æ•°é‡: {len(authors)}")
        print("çƒ­é—¨ä½œè€…:")
        for author, count in sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  - {author}: {count}ç¯‡")
        
        # ç»Ÿè®¡æ ‡ç­¾
        all_tags = []
        for article in self.articles:
            all_tags.extend(article.get('tags', []))
        
        tag_counts = {}
        for tag in all_tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        if tag_counts:
            print("çƒ­é—¨æ ‡ç­¾:")
            for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  - {tag}: {count}æ¬¡")


# ==================== ç¤ºä¾‹ï¼šçˆ¬å–æ¨¡æ‹Ÿæ–°é—»ç½‘ç«™ ====================

def example_simple_crawler():
    """
    ç®€å•ç¤ºä¾‹ï¼šçˆ¬å–ç¤ºä¾‹ç½‘ç«™
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹1ï¼šç®€å•æ–°é—»çˆ¬è™«")
    print("=" * 60 + "\n")
    
    # æ¨¡æ‹Ÿæ–°é—»HTMLï¼ˆå®é™…é¡¹ç›®ä¸­æ˜¯çœŸå®ç½‘ç«™ï¼‰
    sample_html = """
    <div class="news-list">
        <div class="news-item">
            <a href="/news/1" class="title">Python 3.12æ­£å¼å‘å¸ƒ</a>
            <span class="author">æŠ€æœ¯ç¼–è¾‘</span>
            <span class="date">2024-01-15</span>
            <p class="summary">Python 3.12å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡...</p>
        </div>
        <div class="news-item">
            <a href="/news/2" class="title">AIæŠ€æœ¯çªç ´æ€§è¿›å±•</a>
            <span class="author">ç§‘æŠ€è®°è€…</span>
            <span class="date">2024-01-16</span>
            <p class="summary">äººå·¥æ™ºèƒ½é¢†åŸŸè¿æ¥é‡å¤§çªç ´...</p>
        </div>
    </div>
    """
    
    # è§£æç¤ºä¾‹
    soup = BeautifulSoup(sample_html, 'html.parser')
    news_items = soup.find_all('div', class_='news-item')
    
    print(f"æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»:\n")
    for i, item in enumerate(news_items, 1):
        title = item.find('a', class_='title').get_text()
        author = item.find('span', class_='author').get_text()
        date = item.find('span', class_='date').get_text()
        summary = item.find('p', class_='summary').get_text()
        
        print(f"æ–°é—» {i}:")
        print(f"  æ ‡é¢˜: {title}")
        print(f"  ä½œè€…: {author}")
        print(f"  æ—¥æœŸ: {date}")
        print(f"  æ‘˜è¦: {summary}")
        print()


# ==================== å®æˆ˜ï¼šçˆ¬å–çœŸå®ç½‘ç«™ ====================

def example_real_crawler():
    """
    å®æˆ˜ç¤ºä¾‹ï¼šçˆ¬å–ç»ƒä¹ ç½‘ç«™
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹2ï¼šçˆ¬å–çœŸå®ç½‘ç«™ï¼ˆhttp://books.toscrape.comï¼‰")
    print("=" * 60 + "\n")
    
    try:
        url = "http://books.toscrape.com/"
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–ä¹¦ç±ä¿¡æ¯
        books = soup.select('.product_pod')
        
        book_list = []
        for book in books[:5]:  # åªæ˜¾ç¤ºå‰5æœ¬
            title = book.select_one('h3 a')['title']
            price = book.select_one('.price_color').get_text()
            rating = book.select_one('.star-rating')['class'][1]
            link = book.select_one('h3 a')['href']
            
            book_data = {
                'title': title,
                'price': price,
                'rating': rating,
                'link': url + 'catalogue/' + link
            }
            book_list.append(book_data)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"âœ… æˆåŠŸçˆ¬å– {len(book_list)} æœ¬ä¹¦:\n")
        for i, book in enumerate(book_list, 1):
            print(f"ä¹¦ç± {i}:")
            print(f"  æ ‡é¢˜: {book['title']}")
            print(f"  ä»·æ ¼: {book['price']}")
            print(f"  è¯„åˆ†: {book['rating']}")
            print(f"  é“¾æ¥: {book['link']}")
            print()
        
        # ä¿å­˜åˆ°JSON
        output_file = os.path.join(Config.OUTPUT_DIR, "books_sample.json")
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(book_list, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")


# ==================== ç»ƒä¹ é¢˜ ====================

def exercises():
    """
    è¯¾åç»ƒä¹ 
    """
    print("\n" + "=" * 60)
    print("ğŸ“ è¯¾åç»ƒä¹ ")
    print("=" * 60 + "\n")
    
    print("""
ã€ç»ƒä¹ 1ã€‘å®Œå–„æ–°é—»çˆ¬è™«
åŸºäºNewsCrawlerç±»ï¼š
1. æ·»åŠ åˆ†ç±»çˆ¬å–åŠŸèƒ½ï¼ˆç§‘æŠ€ã€è´¢ç»ã€ä½“è‚²ç­‰ï¼‰
2. æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
3. æ·»åŠ å»é‡åŠŸèƒ½ï¼ˆé¿å…é‡å¤çˆ¬å–ï¼‰
4. æ·»åŠ æ—¥å¿—è®°å½•

ã€ç»ƒä¹ 2ã€‘çˆ¬å–ä¸åŒç½‘ç«™
é€‰æ‹©ä»¥ä¸‹ç½‘ç«™ä¹‹ä¸€è¿›è¡Œçˆ¬å–ï¼š
1. http://quotes.toscrape.com/ - åè¨€ç½‘ç«™
2. http://books.toscrape.com/ - å›¾ä¹¦ç½‘ç«™
è¦æ±‚ï¼š
- çˆ¬å–è‡³å°‘3é¡µæ•°æ®
- ä¿å­˜ä¸ºJSONæ ¼å¼
- æ·»åŠ é”™è¯¯å¤„ç†
- æ˜¾ç¤ºçˆ¬å–è¿›åº¦

ã€ç»ƒä¹ 3ã€‘æ•°æ®åˆ†æ
åŸºäºçˆ¬å–çš„æ•°æ®ï¼š
1. ç»Ÿè®¡æœ€å¸¸è§çš„æ ‡ç­¾/åˆ†ç±»
2. æ‰¾å‡ºæœ€çƒ­é—¨çš„ä½œè€…
3. åˆ†æå‘å¸ƒæ—¶é—´åˆ†å¸ƒ
4. ç”Ÿæˆæ•°æ®æŠ¥å‘Š

ã€ç»ƒä¹ 4ã€‘ä¼˜åŒ–çˆ¬è™«
ä¼˜åŒ–æ–¹é¢ï¼š
1. æ·»åŠ æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½
2. å®ç°å¢é‡çˆ¬å–ï¼ˆåªçˆ¬æ–°å†…å®¹ï¼‰
3. æ·»åŠ ä»£ç†IPæ”¯æŒ
4. å®ç°å¹¶å‘çˆ¬å–ï¼ˆå¤šçº¿ç¨‹ï¼‰

æç¤ºï¼š
- å…ˆåˆ†æç½‘ç«™ç»“æ„ï¼ˆF12å¼€å‘è€…å·¥å…·ï¼‰
- ç¡®å®šæ•°æ®åœ¨å“ªä¸ªæ ‡ç­¾é‡Œ
- ä»å°è§„æ¨¡æµ‹è¯•å¼€å§‹
- éµå®ˆç½‘ç«™çš„robots.txtåè®®
    """)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰é˜¶æ®µå®æˆ˜ - æ–°é—»çˆ¬è™«")
    print("=" * 60)
    
    # è¿è¡Œç¤ºä¾‹
    example_simple_crawler()
    example_real_crawler()
    
    # æ˜¾ç¤ºç»ƒä¹ é¢˜
    exercises()
    
    print("\n" + "=" * 60)
    print("âœ… æ–°é—»çˆ¬è™«å­¦ä¹ å®Œæˆï¼")
    print("ğŸ’¡ æ ¸å¿ƒæµç¨‹ï¼š")
    print("   1. çˆ¬å–åˆ—è¡¨é¡µè·å–æ–‡ç« é“¾æ¥")
    print("   2. è¿›å…¥è¯¦æƒ…é¡µè·å–å®Œæ•´å†…å®¹")
    print("   3. æ•°æ®æ¸…æ´—å’Œç»“æ„åŒ–")
    print("   4. ä¿å­˜ä¸ºJSONæ–‡ä»¶")
    print("â­ï¸  ä¸‹ä¸€æ­¥ï¼šå­¦ä¹  02_ecommerce_crawler.py")
    print("=" * 60)


if __name__ == "__main__":
    main()

