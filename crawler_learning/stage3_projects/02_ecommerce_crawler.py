"""
ç¬¬ä¸‰é˜¶æ®µå®æˆ˜ - ç”µå•†çˆ¬è™«

é¡¹ç›®ï¼šçˆ¬å–ç”µå•†ç½‘ç«™çš„å•†å“ä¿¡æ¯
æŠ€æœ¯ï¼šrequests + XPath + æ•°æ®åˆ†æ
"""

import requests
from lxml import etree
import json
import csv
import time
from typing import List, Dict
import os
from urllib.parse import urljoin

# ==================== ç”µå•†çˆ¬è™«ç±» ====================

class EcommerceCrawler:
    """
    ç”µå•†çˆ¬è™«ç±»
    
    åŠŸèƒ½ï¼š
    1. çˆ¬å–å•†å“åˆ—è¡¨
    2. æå–å•†å“è¯¦ç»†ä¿¡æ¯
    3. ä¸‹è½½å•†å“å›¾ç‰‡
    4. æ•°æ®åˆ†æç»Ÿè®¡
    """
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        })
        self.products = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = "ecommerce_data"
        self.images_dir = os.path.join(self.output_dir, "images")
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.images_dir, exist_ok=True)
    
    def fetch(self, url: str, max_retries: int = 3) -> str:
        """è·å–ç½‘é¡µå†…å®¹"""
        for i in range(max_retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                return response.text
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ˆ{i+1}/{max_retries}ï¼‰: {e}")
                if i < max_retries - 1:
                    time.sleep(2)
        return ""
    
    def parse_product_list(self, html: str) -> List[Dict]:
        """
        è§£æå•†å“åˆ—è¡¨é¡µ
        """
        tree = etree.HTML(html)
        products = []
        
        # ä½¿ç”¨XPathæå–å•†å“ä¿¡æ¯
        # è¿™é‡Œä»¥books.toscrape.comä¸ºä¾‹
        product_nodes = tree.xpath('//article[@class="product_pod"]')
        
        for node in product_nodes:
            try:
                # æ ‡é¢˜
                title = node.xpath('.//h3/a/@title')
                title = title[0] if title else ""
                
                # ä»·æ ¼
                price = node.xpath('.//p[@class="price_color"]/text()')
                price = price[0] if price else "0"
                
                # è¯„åˆ†
                rating_class = node.xpath('.//p[contains(@class, "star-rating")]/@class')
                if rating_class:
                    rating = rating_class[0].split()[-1]
                else:
                    rating = "Unknown"
                
                # é“¾æ¥
                link = node.xpath('.//h3/a/@href')
                link = urljoin(self.base_url, link[0]) if link else ""
                
                # å›¾ç‰‡
                image = node.xpath('.//img/@src')
                image = urljoin(self.base_url, image[0]) if image else ""
                
                # åº“å­˜
                stock = node.xpath('.//p[@class="instock availability"]/text()')
                stock = stock[1].strip() if len(stock) > 1 else "Unknown"
                
                product = {
                    'title': title,
                    'price': price,
                    'rating': rating,
                    'link': link,
                    'image': image,
                    'stock': stock
                }
                
                products.append(product)
                
            except Exception as e:
                print(f"âš ï¸ è§£æå•†å“å¤±è´¥: {e}")
                continue
        
        return products
    
    def download_image(self, url: str, filename: str) -> bool:
        """
        ä¸‹è½½å•†å“å›¾ç‰‡
        """
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            filepath = os.path.join(self.images_dir, filename)
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            return True
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å›¾ç‰‡å¤±è´¥ {url}: {e}")
            return False
    
    def crawl(self, start_page: int = 1, max_pages: int = 3):
        """
        æ‰§è¡Œçˆ¬å–ä»»åŠ¡
        """
        print("=" * 60)
        print("ğŸ›’ å¼€å§‹çˆ¬å–å•†å“ä¿¡æ¯")
        print("=" * 60)
        
        for page in range(start_page, start_page + max_pages):
            print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # æ„é€ åˆ†é¡µURL
            if page == 1:
                url = self.base_url
            else:
                url = f"{self.base_url}/catalogue/page-{page}.html"
            
            # è·å–é¡µé¢
            html = self.fetch(url)
            if not html:
                print(f"âŒ è·å–ç¬¬ {page} é¡µå¤±è´¥")
                continue
            
            # è§£æå•†å“
            products = self.parse_product_list(html)
            print(f"âœ… æ‰¾åˆ° {len(products)} ä¸ªå•†å“")
            
            # ä¸‹è½½å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
            for i, product in enumerate(products, 1):
                self.products.append(product)
                
                # ä¸‹è½½å›¾ç‰‡
                if product['image']:
                    img_filename = f"product_{len(self.products)}.jpg"
                    if self.download_image(product['image'], img_filename):
                        product['local_image'] = img_filename
                        print(f"  [{i}/{len(products)}] âœ… {product['title'][:30]}...")
                    else:
                        product['local_image'] = ""
                        print(f"  [{i}/{len(products)}] âš ï¸ {product['title'][:30]}... (å›¾ç‰‡ä¸‹è½½å¤±è´¥)")
                
                time.sleep(0.5)  # å»¶è¿Ÿ
            
            time.sleep(1)  # é¡µé¢é—´å»¶è¿Ÿ
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.products)} ä¸ªå•†å“")
    
    def save_to_json(self, filename: str = "products.json"):
        """ä¿å­˜ä¸ºJSON"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.products, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
    
    def save_to_csv(self, filename: str = "products.csv"):
        """ä¿å­˜ä¸ºCSV"""
        if not self.products:
            print("âš ï¸ æ²¡æœ‰æ•°æ®")
            return
        
        filepath = os.path.join(self.output_dir, filename)
        
        # è·å–æ‰€æœ‰å­—æ®µ
        fieldnames = list(self.products[0].keys())
        
        with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.products)
        
        print(f"âœ… CSVæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
    
    def analyze(self):
        """æ•°æ®åˆ†æ"""
        if not self.products:
            print("âš ï¸ æ²¡æœ‰æ•°æ®")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®åˆ†æ")
        print("=" * 60)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"å•†å“æ€»æ•°: {len(self.products)}")
        
        # ä»·æ ¼ç»Ÿè®¡
        prices = []
        for p in self.products:
            price_str = p.get('price', 'Â£0').replace('Â£', '').replace('$', '')
            try:
                price = float(price_str)
                prices.append(price)
            except:
                continue
        
        if prices:
            avg_price = sum(prices) / len(prices)
            max_price = max(prices)
            min_price = min(prices)
            
            print(f"\nä»·æ ¼ç»Ÿè®¡:")
            print(f"  å¹³å‡ä»·æ ¼: Â£{avg_price:.2f}")
            print(f"  æœ€é«˜ä»·æ ¼: Â£{max_price:.2f}")
            print(f"  æœ€ä½ä»·æ ¼: Â£{min_price:.2f}")
            
            # æ‰¾å‡ºæœ€è´µå’Œæœ€ä¾¿å®œçš„å•†å“
            for p in self.products:
                price_str = p.get('price', 'Â£0').replace('Â£', '').replace('$', '')
                try:
                    if float(price_str) == max_price:
                        print(f"  æœ€è´µå•†å“: {p['title']} - {p['price']}")
                    if float(price_str) == min_price:
                        print(f"  æœ€ä¾¿å®œ: {p['title']} - {p['price']}")
                except:
                    pass
        
        # è¯„åˆ†ç»Ÿè®¡
        ratings = {}
        for p in self.products:
            rating = p.get('rating', 'Unknown')
            ratings[rating] = ratings.get(rating, 0) + 1
        
        print(f"\nè¯„åˆ†åˆ†å¸ƒ:")
        rating_order = ['Five', 'Four', 'Three', 'Two', 'One']
        for rating in rating_order:
            count = ratings.get(rating, 0)
            percentage = (count / len(self.products)) * 100
            print(f"  {rating} Star: {count} ({percentage:.1f}%)")
        
        # åº“å­˜ç»Ÿè®¡
        in_stock = sum(1 for p in self.products if 'In stock' in p.get('stock', ''))
        print(f"\nåº“å­˜ç»Ÿè®¡:")
        print(f"  æœ‰è´§: {in_stock}")
        print(f"  æ— è´§: {len(self.products) - in_stock}")


# ==================== ç¤ºä¾‹ä½¿ç”¨ ====================

def example_ecommerce():
    """
    ç¤ºä¾‹ï¼šçˆ¬å–ç”µå•†ç½‘ç«™
    """
    print("\n" + "=" * 60)
    print("ç”µå•†çˆ¬è™«å®æˆ˜ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = EcommerceCrawler("http://books.toscrape.com")
    
    # çˆ¬å–æ•°æ®
    crawler.crawl(start_page=1, max_pages=2)
    
    # æ•°æ®åˆ†æ
    crawler.analyze()
    
    # ä¿å­˜æ•°æ®
    crawler.save_to_json()
    crawler.save_to_csv()
    
    print("\nâœ… å®Œæˆï¼")


# ==================== ç»ƒä¹ é¢˜ ====================

def exercises():
    """
    è¯¾åç»ƒä¹ 
    """
    print("\n" + "=" * 60)
    print("ğŸ“ è¯¾åç»ƒä¹ ")
    print("=" * 60 + "\n")
    
    print("""
ã€ç»ƒä¹ 1ã€‘å¢å¼ºçˆ¬è™«åŠŸèƒ½
åœ¨EcommerceCrawleråŸºç¡€ä¸Šæ·»åŠ ï¼š
1. å•†å“è¯¦æƒ…é¡µçˆ¬å–ï¼ˆæ›´å¤šä¿¡æ¯ï¼‰
2. å•†å“åˆ†ç±»çˆ¬å–
3. ä»·æ ¼å˜åŠ¨ç›‘æ§
4. è¯„è®ºçˆ¬å–

ã€ç»ƒä¹ 2ã€‘æ•°æ®å¯è§†åŒ–
ä½¿ç”¨matplotlibæˆ–pandasï¼š
1. ç»˜åˆ¶ä»·æ ¼åˆ†å¸ƒå›¾
2. ç»˜åˆ¶è¯„åˆ†é¥¼å›¾
3. ç”Ÿæˆå•†å“å¯¹æ¯”è¡¨
4. å¯¼å‡ºExcelæŠ¥å‘Š

ã€ç»ƒä¹ 3ã€‘æ™ºèƒ½åˆ†æ
å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
1. æ€§ä»·æ¯”æ’åºï¼ˆè¯„åˆ†/ä»·æ ¼ï¼‰
2. çƒ­é”€å•†å“è¯†åˆ«
3. ä»·æ ¼å¼‚å¸¸æ£€æµ‹
4. æ¨èç®—æ³•å®ç°

ã€ç»ƒä¹ 4ã€‘çˆ¬è™«ä¼˜åŒ–
ä¼˜åŒ–é¡¹ï¼š
1. å®ç°æ–­ç‚¹ç»­çˆ¬
2. æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤º
3. æ”¯æŒå¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡
4. æ·»åŠ æ•°æ®å»é‡

æç¤ºï¼š
- ä½¿ç”¨pandaså¤„ç†æ•°æ®æ›´æ–¹ä¾¿
- matplotlibå¯ä»¥ç»˜åˆ¶å›¾è¡¨
- è€ƒè™‘æ·»åŠ æ—¥å¿—ç³»ç»Ÿ
- éµå®ˆç½‘ç«™çˆ¬è™«åè®®
    """)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰é˜¶æ®µå®æˆ˜ - ç”µå•†çˆ¬è™«")
    print("=" * 60)
    
    # è¿è¡Œç¤ºä¾‹
    example_ecommerce()
    
    # æ˜¾ç¤ºç»ƒä¹ é¢˜
    exercises()
    
    print("\n" + "=" * 60)
    print("âœ… ç”µå•†çˆ¬è™«å­¦ä¹ å®Œæˆï¼")
    print("ğŸ’¡ æ ¸å¿ƒè¦ç‚¹ï¼š")
    print("   1. ä½¿ç”¨XPathç²¾ç¡®æå–æ•°æ®")
    print("   2. æ‰¹é‡ä¸‹è½½å›¾ç‰‡èµ„æº")
    print("   3. å¤šæ ¼å¼æ•°æ®å­˜å‚¨ï¼ˆJSON/CSVï¼‰")
    print("   4. æ•°æ®ç»Ÿè®¡åˆ†æ")
    print("â­ï¸  ä¸‹ä¸€æ­¥ï¼šå­¦ä¹  03_data_storage.py")
    print("=" * 60)


if __name__ == "__main__":
    main()

